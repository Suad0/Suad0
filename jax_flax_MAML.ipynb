{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRL6OYVIzhz2K6FoXuvz3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suad0/Suad0/blob/main/jax_flax_MAML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY0Kx8o_7NhO",
        "outputId": "5a1cc18a-e9e2-444a-fd1c-3cf0a5b4a310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (0.2.5)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (4.9.9)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax) (1.15.3)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax) (0.1.89)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.7.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.12.2)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (5.29.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (18.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (2.32.3)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (3.1.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets) (1.17.2)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax) (4.14.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2025.6.15)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow-datasets) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.70.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install jax jaxlib optax tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "import jax.random as random\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, vmap, random, jit\n",
        "import tensorflow_datasets as tfds\n",
        "import optax\n",
        "import numpy as np\n",
        "\n",
        "rng_key = random.PRNGKey(0)\n",
        "\n",
        "# Hyperparameters\n",
        "inner_steps = 5\n",
        "inner_lr = 0.1\n",
        "outer_lr = 0.001\n",
        "k_shot = 5  # 5-shot learning\n",
        "n_way = 5   # 5-way classification\n",
        "batch_tasks = 4  # Number of tasks per meta-batch\n",
        "img_size = (28, 28, 1)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_omniglot():\n",
        "    ds, info = tfds.load('omniglot', split='train', with_info=True, as_supervised=True)\n",
        "\n",
        "    # Fetch all data as NumPy arrays\n",
        "    images_np = []\n",
        "    labels_np = []\n",
        "    for image, label in tfds.as_numpy(ds):\n",
        "        images_np.append(image)\n",
        "        labels_np.append(label)\n",
        "\n",
        "    images_np = jnp.array(images_np)\n",
        "    labels_np = jnp.array(labels_np)\n",
        "\n",
        "    def preprocess(image):\n",
        "        image = image.astype(jnp.float32) / 255.0\n",
        "        image = jnp.resize(image, img_size)\n",
        "        return image\n",
        "\n",
        "    # Apply preprocessing to the NumPy arrays\n",
        "    images_processed = vmap(preprocess)(images_np)\n",
        "\n",
        "    # Create a JAX-compatible dataset (list of tuples or similar)\n",
        "    # For simplicity, let's return the processed arrays directly for now\n",
        "    return images_processed, labels_np, info\n",
        "\n",
        "def init_model_params(rng, input_shape):\n",
        "    def conv_layer(rng, in_channels, out_channels):\n",
        "        w_key, b_key = random.split(rng)\n",
        "        # Fixed: Use proper conv weight shape (out_channels, in_channels, kernel_h, kernel_w)\n",
        "        w = random.normal(w_key, (out_channels, in_channels, 3, 3)) * 0.01\n",
        "        b = jnp.zeros((out_channels,))\n",
        "        return {'w': w, 'b': b}\n",
        "\n",
        "    params = []\n",
        "    rng, *layer_rngs = random.split(rng, 4)\n",
        "    params.append(conv_layer(layer_rngs[0], input_shape[-1], 64))\n",
        "    params.append(conv_layer(layer_rngs[1], 64, 64))\n",
        "    params.append({'w': random.normal(layer_rngs[2], (64, n_way)) * 0.01, 'b': jnp.zeros((n_way,))})\n",
        "    return params\n",
        "\n",
        "# Fixed forward pass\n",
        "def forward(params, x):\n",
        "    # x should have shape (batch_size, height, width, channels)\n",
        "    # Ensure x has the right shape\n",
        "    if x.ndim == 3:\n",
        "        x = x[None, ...]  # Add batch dimension if missing\n",
        "\n",
        "    # Conv layer 1\n",
        "    x = jax.lax.conv_general_dilated(\n",
        "        x, params[0]['w'],\n",
        "        window_strides=(1, 1),\n",
        "        padding='SAME',\n",
        "        dimension_numbers=('NHWC', 'OIHW', 'NHWC')\n",
        "    )\n",
        "    x = x + params[0]['b'][None, None, None, :]  # Add bias\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    # Conv layer 2\n",
        "    x = jax.lax.conv_general_dilated(\n",
        "        x, params[1]['w'],\n",
        "        window_strides=(1, 1),\n",
        "        padding='SAME',\n",
        "        dimension_numbers=('NHWC', 'OIHW', 'NHWC')\n",
        "    )\n",
        "    x = x + params[1]['b'][None, None, None, :]  # Add bias\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    # Global average pooling\n",
        "    x = jnp.mean(x, axis=(1, 2))  # Shape: (batch_size, channels)\n",
        "\n",
        "    # Final linear layer\n",
        "    logits = jnp.dot(x, params[2]['w']) + params[2]['b']\n",
        "    return logits\n",
        "\n",
        "# Loss function\n",
        "def loss_fn(params, x, y):\n",
        "    logits = forward(params, x)\n",
        "    return -jnp.mean(jax.nn.log_softmax(logits)[jnp.arange(y.shape[0]), y])\n",
        "\n",
        "# Inner loop update\n",
        "@jit\n",
        "def inner_update(params, x_supp, y_supp):\n",
        "    grads = grad(loss_fn)(params, x_supp, y_supp)\n",
        "    updated_params = []\n",
        "    for p, g in zip(params, grads):\n",
        "        if isinstance(p, dict):\n",
        "            updated_p = {}\n",
        "            for key in p:\n",
        "                updated_p[key] = p[key] - inner_lr * g[key]\n",
        "            updated_params.append(updated_p)\n",
        "        else:\n",
        "            updated_params.append(p - inner_lr * g)\n",
        "    return updated_params\n",
        "\n",
        "# MAML inner loop\n",
        "@jit\n",
        "def maml_inner(params, x_supp, y_supp, x_query, y_query):\n",
        "    adapted_params = params\n",
        "    for _ in range(inner_steps):\n",
        "        adapted_params = inner_update(adapted_params, x_supp, y_supp)\n",
        "    return loss_fn(adapted_params, x_query, y_query)\n",
        "\n",
        "# Meta-loss over a batch of tasks\n",
        "@jit\n",
        "def meta_loss(params, batch):\n",
        "    losses = vmap(maml_inner, in_axes=(None, 0, 0, 0, 0))(\n",
        "        params, batch['x_supp'], batch['y_supp'], batch['x_query'], batch['y_query']\n",
        "    )\n",
        "    return jnp.mean(losses)\n",
        "\n",
        "# Meta-gradient computation\n",
        "meta_grad = jit(grad(meta_loss))\n",
        "\n",
        "# Training loop\n",
        "def train_step(params, batch, optimizer, opt_state):\n",
        "    grads = meta_grad(params, batch)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state\n",
        "\n",
        "# Prepare task batch\n",
        "def prepare_task_batch(images, labels, rng, num_tasks, k_shot, n_way):\n",
        "    batch = {'x_supp': [], 'y_supp': [], 'x_query': [], 'y_query': []}\n",
        "    num_classes = int(labels.max()) + 1  # Assuming labels are 0-indexed\n",
        "\n",
        "    # Group images and labels by class\n",
        "    class_images = [[] for _ in range(num_classes)]\n",
        "    for i, label in enumerate(labels):\n",
        "        class_images[int(label)].append(images[i])\n",
        "\n",
        "    # Convert to arrays and filter out empty classes\n",
        "    class_images = [jnp.array(c) for c in class_images if len(c) >= 2 * k_shot]\n",
        "    available_classes = len(class_images)\n",
        "\n",
        "    if available_classes < n_way:\n",
        "        raise ValueError(f\"Not enough classes with sufficient samples. Need {n_way}, got {available_classes}\")\n",
        "\n",
        "    for _ in range(num_tasks):\n",
        "        rng, class_rng = jax.random.split(rng)\n",
        "        # Sample n_way classes\n",
        "        sampled_class_indices = jax.random.choice(\n",
        "            class_rng, available_classes, shape=(n_way,), replace=False\n",
        "        )\n",
        "\n",
        "        task_x_supp = []\n",
        "        task_y_supp = []\n",
        "        task_x_query = []\n",
        "        task_y_query = []\n",
        "\n",
        "        for new_label, class_idx in enumerate(sampled_class_indices):\n",
        "            rng, sample_rng = jax.random.split(rng)\n",
        "            class_data = class_images[int(class_idx)]\n",
        "\n",
        "            # Sample k_shot support and k_shot query examples from each class\n",
        "            class_indices = jax.random.permutation(sample_rng, len(class_data))\n",
        "            supp_indices = class_indices[:k_shot]\n",
        "            query_indices = class_indices[k_shot:2 * k_shot]\n",
        "\n",
        "            task_x_supp.append(class_data[supp_indices])\n",
        "            task_y_supp.append(jnp.full((k_shot,), new_label))  # Use new labels 0, 1, 2, ..., n_way-1\n",
        "            task_x_query.append(class_data[query_indices])\n",
        "            task_y_query.append(jnp.full((k_shot,), new_label))\n",
        "\n",
        "        batch['x_supp'].append(jnp.concatenate(task_x_supp, axis=0))\n",
        "        batch['y_supp'].append(jnp.concatenate(task_y_supp, axis=0))\n",
        "        batch['x_query'].append(jnp.concatenate(task_x_query, axis=0))\n",
        "        batch['y_query'].append(jnp.concatenate(task_y_query, axis=0))\n",
        "\n",
        "    return {k: jnp.array(v) for k, v in batch.items()}\n",
        "\n",
        "# Main training\n",
        "def main(rng_key):\n",
        "    images_np, labels_np, _ = load_omniglot()\n",
        "    params = init_model_params(rng_key, img_size)\n",
        "    optimizer = optax.adam(outer_lr)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    for step in range(10):\n",
        "        rng_key, subkey = random.split(rng_key)\n",
        "        try:\n",
        "            batch = prepare_task_batch(images_np, labels_np, subkey, batch_tasks, k_shot, n_way)\n",
        "            params, opt_state = train_step(params, batch, optimizer, opt_state)\n",
        "            if step % 100 == 0:\n",
        "                loss = meta_loss(params, batch)\n",
        "                print(f\"Step {step}, Meta-Loss: {loss:.4f}\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Error at step {step}: {e}\")\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(rng_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMrVygMPPM49",
        "outputId": "9b5669bd-70ae-43a8-db6f-81328573547e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Meta-Loss: 1.6094\n"
          ]
        }
      ]
    }
  ]
}